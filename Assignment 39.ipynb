{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a6ceb82c-7f1f-44b0-8ba1-d39134a0c695",
   "metadata": {},
   "source": [
    "## Q1: What are missing values in a dataset? Why is it essential to handle missing values? Name some algorithms that are not affected by missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7e9166-8f2a-49aa-8f1d-f237726bc207",
   "metadata": {},
   "source": [
    "Missing values: Represent absent data points in a dataset, appearing as blanks, null values, or specific codes.\n",
    "\n",
    "Handling missing values is crucial because they can:\n",
    "\n",
    "Bias models towards observations with complete data.\n",
    "\n",
    "Increase variance, making the model less stable.\n",
    "\n",
    "Reduce the effectiveness of some machine learning algorithms.\n",
    "\n",
    "Algorithms not affected by missing values:\n",
    "\n",
    "Decision trees: Split data based on existing features, ignoring missing values.\n",
    "\n",
    "K-nearest neighbors: Uses similarity to existing data points, ignoring missing features.\n",
    "\n",
    "Support Vector Machines (SVMs): Focus on a small number of data points (support vectors), potentially unaffected by missing values in others."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d15cd500-b8f0-4db6-a9a5-28c6bd60ec84",
   "metadata": {},
   "source": [
    "## Q2: List down techniques used to handle missing data. Give an example of each with python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40b3d547-3b73-4cdd-8eda-636564488d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "3  4.0  8.0\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "'''Deletion:\n",
    "\n",
    "Simple and efficient but can discard potentially valuable information.\n",
    "Options:\n",
    "dropna(): Drop rows/columns with missing values:\n",
    "Python'''\n",
    "import pandas as pd\n",
    "\n",
    "data = {'A': [1, 2, None, 4], 'B': [5, None, 7, 8]}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "df_dropna = df.dropna()  # Drops entire rows with missing values\n",
    "print(df_dropna)\n",
    "\n",
    "df_dropna_col = df.dropna(axis=1)  # Drops column 'B' with missing values\n",
    "print(df_dropna_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a501a923-3479-4070-8bdf-30a76542f55a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          A         B\n",
      "0  1.000000  5.000000\n",
      "1  2.000000  2.333333\n",
      "2  2.333333  7.000000\n",
      "3  4.000000  8.000000\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  7.0\n",
      "2  7.0  7.0\n",
      "3  4.0  8.0\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  1.0\n",
      "2  1.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "'''2. Imputation:\n",
    "\n",
    "Fills missing values with estimated values based on existing data.\n",
    "Options:\n",
    "Mean/Median/Mode: Replace missing values with the mean, median, or most frequent value of the column.\n",
    "Python'''\n",
    "df_mean = df.fillna(df['A'].mean())  # Fill missing values with mean of column 'A'\n",
    "print(df_mean)\n",
    "\n",
    "df_median = df.fillna(df['B'].median())  # Fill missing values with median of column 'B'\n",
    "print(df_median)\n",
    "\n",
    "df_mode = df.fillna(df['A'].mode().iloc[0])  # Fill missing values with mode of column 'A'\n",
    "print(df_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a648ab5d-ca9f-4b8b-83f4-c5bf0642c9d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  5.0\n",
      "2  2.0  7.0\n",
      "3  4.0  8.0\n",
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  7.0\n",
      "2  4.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "'''3. Forward Fill/Backward Fill:\n",
    "\n",
    "Fills missing values with the value from the previous (forward) or next (backward) non-missing value in the same column.\n",
    "Python'''\n",
    "df_ffill = df.fillna(method='ffill')  # Forward fill missing values\n",
    "print(df_ffill)\n",
    "\n",
    "df_bfill = df.fillna(method='bfill')  # Backward fill missing values\n",
    "print(df_bfill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "674840c2-1c44-46ef-aed0-b67ae07a38b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     A    B\n",
      "0  1.0  5.0\n",
      "1  2.0  6.0\n",
      "2  3.0  7.0\n",
      "3  4.0  8.0\n"
     ]
    }
   ],
   "source": [
    "'''4. Interpolation:\n",
    "\n",
    "Estimates missing values based on relationships between existing data points.\n",
    "Useful for numerical data with a clear trend.\n",
    "Python'''\n",
    "\n",
    "df_interp = df.interpolate('linear')  # Linear interpolation\n",
    "print(df_interp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d533d0a9-238c-4816-96b0-e8a54861e34e",
   "metadata": {},
   "source": [
    "## Q3: Explain the imbalanced data. What will happen if imbalanced data is not handled?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f06c22b-6a88-4e72-ac8f-5f2069ea8dbf",
   "metadata": {},
   "source": [
    "Imbalanced Data: Refers to datasets where one or more classes (categories) are significantly outnumbered by the others.\n",
    "\n",
    "For example, in fraud detection, the vast majority of transactions are likely to be legitimate, with only a tiny fraction being fraudulent.\n",
    "This creates an inherent class imbalance.\n",
    "\n",
    "What happens if not handled:\n",
    "\n",
    "Poor Performance on Minority Class: Standard machine learning algorithms often prioritize the majority class, as optimizing for overall accuracy can lead to high scores even if the minority class is poorly classified. This is problematic if the minority class is the one you care about most (e.g., the fraudulent transactions).\n",
    "\n",
    "Biased Models: Models trained on imbalanced data learn to associate majority class features with the target variable, leading to bias towards the majority class. Such models can incorrectly predict the majority class even when the true outcome should have been the minority one.\n",
    "\n",
    "Misleading Evaluation Metrics: Standard metrics like accuracy can be deceptive in imbalanced cases. A model achieving 99% accuracy in fraud detection might seem excellent, but this could mean it's simply predicting all instances as non-fraud, entirely missing the fraud cases."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d525a9-862c-46b1-8438-69775c88d980",
   "metadata": {},
   "source": [
    "## Q4: What are Up-sampling and Down-sampling? Explain with an example when up-sampling and down- sampling are required."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb157a68-8e19-4419-a972-57bbb5044348",
   "metadata": {},
   "source": [
    "1. Up-sampling:\n",
    "\n",
    "Increases the representation of the minority class in the dataset.\n",
    "\n",
    "Methods:\n",
    "Replication: Duplicating existing minority class data points.\n",
    "SMOTE (Synthetic Minority Oversampling Technique): Creates synthetic data points for the minority class based on existing data.\n",
    "\n",
    "Example: Imagine a fraud detection dataset with only 1% of transactions being fraudulent. Up-sampling could be used to duplicate minority class samples (fraudulent transactions) to achieve a more balanced representation.\n",
    "\n",
    "2. Down-sampling:\n",
    "\n",
    "Reduces the representation of the majority class to match the size of the minority class.\n",
    "\n",
    "Methods:\n",
    "\n",
    "Random sampling: Randomly selecting data points from the majority class until it matches the minority class size.\n",
    "\n",
    "Stratified sampling: Ensures the distribution of features within the majority class sample is similar to the original distribution.\n",
    "\n",
    "Example: Continuing the fraud detection scenario, down-sampling might involve randomly removing instances from the majority class (non-fraudulent transactions) to match the number of fraudulent transactions.\n",
    "\n",
    "Choosing between Up-sampling and Down-sampling:\n",
    "\n",
    "Up-sampling is preferred when the minority class data is limited and losing information is undesirable. However, it can lead to overfitting if not done carefully.\n",
    "Down-sampling is suitable when the majority class data is large and computational resources are limited. However, it discards potentially valuable information from the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40497747-0011-45c7-81be-558221b18bba",
   "metadata": {},
   "source": [
    "## Q5: What is data Augmentation? Explain SMOTE."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b6e19f2-caf8-48c2-a29e-dc3a1b765e4b",
   "metadata": {},
   "source": [
    "Data Augmentation:\n",
    "\n",
    "A technique to artificially increase the size and diversity of training datasets, especially for image-based tasks.\n",
    "\n",
    "Involves creating variations of existing data through transformations like rotation, flipping, cropping, or color adjustments.\n",
    "\n",
    "Improves model performance and robustness by reducing overfitting and preventing the model from memorizing specific examples.\n",
    "\n",
    "SMOTE (Synthetic Minority Over-sampling Technique):\n",
    "\n",
    "A data augmentation technique specifically for imbalanced datasets, often tabular data.\n",
    "\n",
    "Generates new synthetic samples for the minority class based on similarities between existing minority class data points.\n",
    "\n",
    "Increases the representation of the minority class, improving the model's ability to learn its patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f69b23-2c70-4419-ba75-9e3dd1cabcf4",
   "metadata": {},
   "source": [
    "## Q6: What are outliers in a dataset? Why is it essential to handle outliers?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d196bf-d4f9-41c7-9dbf-c9e860e3db6b",
   "metadata": {},
   "source": [
    "Outliers: Data points that deviate significantly from the majority of the data in a dataset. They appear as distant from the central tendency (mean, median) and can be much larger or smaller than other values.\n",
    "\n",
    "Handling outliers is important because:\n",
    "\n",
    "They can distort statistical measures: Outliers can significantly influence calculations like mean, skewing the overall picture of the data.\n",
    "\n",
    "They can negatively impact machine learning models: Outliers can confuse models, leading to inaccurate predictions and biased results.\n",
    "\n",
    "They might indicate underlying issues: Sometimes, outliers represent errors in data collection or measurement, requiring investigation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fae590-ff29-4af6-b520-ff2a3ac97bbf",
   "metadata": {},
   "source": [
    "## Q7: You are working on a project that requires analyzing customer data. However, you notice that some of the data is missing. What are some techniques you can use to handle the missing data in your analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e85f9-c449-4c5c-8992-fb64196a3d18",
   "metadata": {},
   "source": [
    "1. Identify the missing data:\n",
    "\n",
    "Understand the extent of missing values (number of entries, percentage) and which variables are affected.\n",
    "\n",
    "2. Choose a suitable handling technique:\n",
    "\n",
    "Deletion: Simplest approach, but can discard valuable information and potentially bias the analysis. Use with caution, especially if data is scarce.\n",
    "\n",
    "Imputation: Fills missing values with estimated values based on existing data.\n",
    "\n",
    "Mean/Median/Mode: Replace with average, middle value, or most frequent value of the column (suitable for numerical and categorical data, respectively).\n",
    "\n",
    "Model-based: Use statistical models like regression to predict missing values based on other features (requires additional resources and expertise).\n",
    "Forward Fill/Backward Fill: Replaces missing values with the value from the previous (forward) or next (backward) non-missing value in the same column (assumes missing value is similar to surrounding ones).\n",
    "\n",
    "Interpolation: Estimates missing values based on mathematical techniques like linear interpolation, suitable for numerical data with a clear trend.\n",
    "\n",
    "3. Consider additional factors:\n",
    "\n",
    "Data type: Techniques like interpolation work best for numerical data, while others might be better suited for categorical data.\n",
    "\n",
    "Data distribution: The distribution of the data can influence the effectiveness of certain imputation methods.\n",
    "\n",
    "Modeling goals: If you plan to use the data for machine learning, choose a technique that minimizes bias and maintains data integrity.\n",
    "\n",
    "4. Evaluate the impact:\n",
    "\n",
    "Compare the results with and without handling missing data to assess if the chosen technique introduces bias or significantly changes the analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2caac8f-ee33-4415-b000-70c460e0aaa7",
   "metadata": {},
   "source": [
    "## Q8: You are working with a large dataset and find that a small percentage of the data is missing. What are some strategies you can use to determine if the missing data is missing at random or if there is a pattern to the missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd24a455-7936-4703-9a81-176d57cab7b1",
   "metadata": {},
   "source": [
    "1. Data Exploration and Visualization:\n",
    "\n",
    "Analyze missingness patterns: Look for systematic trends in how data is missing. Consider:\n",
    "\n",
    "Missingness by variable: Are specific variables missing more than others? Is there a correlation between missing values in different variables?\n",
    "\n",
    "Missingness by group: Does the missingness differ across different groups or categories within the data (e.g., customer demographics, product types)?\n",
    "\n",
    "Visualizations: Create heatmaps or boxplots to visually identify patterns in missingness across variables and groups.\n",
    "\n",
    "2. Statistical Tests:\n",
    "\n",
    "Little's MCAR test: A statistical test specifically designed to assess whether missingness is completely random. It utilizes chi-squared statistics to \n",
    "compare the distribution of variables with missing values to those without.\n",
    "\n",
    "Chi-squared test for independence: Can be used to test the association between missingness in a variable and other categorical variables in the dataset.\n",
    "\n",
    "3. Domain Knowledge and Context:\n",
    "\n",
    "Understanding the data collection process: Consider potential reasons for missing values. Were there specific events or limitations during data collection that might have caused non-random missingness?\n",
    "\n",
    "Subject matter expertise: Leverage your knowledge of the domain and the data sources to identify potential causes for non-random missingness based on real-world context.\n",
    "\n",
    "4. Comparing Complete and Incomplete Cases:\n",
    "\n",
    "Descriptive statistics: Compare the mean, median, standard deviation, and other summary statistics of variables for complete and incomplete cases. Significant differences might indicate non-random missingness.\n",
    "\n",
    "Modeling: Build separate models using only complete and complete-and-imputed data. If the model performance significantly differs, it suggests non-random missingness might be affecting the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c358eb01-f55a-4c49-9368-ed3ed8d29024",
   "metadata": {},
   "source": [
    "## Q9: Suppose you are working on a medical diagnosis project and find that the majority of patients in the dataset do not have the condition of interest, while a small percentage do. What are some strategies you can use to evaluate the performance of your machine learning model on this imbalanced dataset?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0973290-b8d1-4f78-840d-b0d0183ca26f",
   "metadata": {},
   "source": [
    "Evaluating the performance of a machine learning model on an imbalanced dataset, especially a medical diagnosis scenario where the target condition is rare, requires specific metrics and strategies:\n",
    "\n",
    "Metrics:\n",
    "\n",
    "Standard accuracy can be misleading, as the model might simply predict the majority class (no condition) most of the time, even if it performs poorly in identifying the rare class (condition).\n",
    "\n",
    "Precision, Recall, and F1-score:\n",
    "\n",
    "Precision: Measures the proportion of true positives among all positive predictions (avoiding false positives).\n",
    "\n",
    "Recall: Measures the proportion of true positives identified out of all actual positive cases (avoiding false negatives).\n",
    "\n",
    "F1-score: Combines precision and recall into a single metric, providing a balanced view of performance.\n",
    "\n",
    "Area Under the ROC Curve (AUC): Measures the model's ability to distinguish between positive and negative cases, independent of class imbalance.\n",
    "\n",
    "Cost-sensitive metrics: Assign higher weights to misclassifying the minority class (condition), reflecting the higher cost associated with missing a case.\n",
    "\n",
    "Strategies:\n",
    "\n",
    "Stratified evaluation: Ensure the test set maintains the class imbalance ratio present in the original dataset, allowing for a fairer evaluation of the model's performance on both classes.\n",
    "\n",
    "Visualization techniques: Utilize confusion matrices or ROC curves to visualize the model's performance for both majority and minority classes.\n",
    "\n",
    "Compare with baseline models: Compare your model's performance with simpler approaches (e.g., predicting the majority class) to assess if it offers any significant improvement in identifying the rare class.\n",
    "\n",
    "Additional considerations:\n",
    "\n",
    "Data augmentation: Techniques like SMOTE can be used to generate synthetic data points for the minority class, improving the model's ability to learn \n",
    "its patterns.\n",
    "\n",
    "Cost-sensitive learning: Train the model with higher penalties for misclassifying the minority class, encouraging the model to prioritize accurate identification of the rare condition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0737ece7-15ff-40b7-bf2e-66be14c174b1",
   "metadata": {},
   "source": [
    "## Q10: When attempting to estimate customer satisfaction for a project, you discover that the dataset is unbalanced, with the bulk of customers reporting being satisfied. What methods can you employ to balance the dataset and down-sample the majority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc733ce-a7f6-4e36-9a28-d3134e9cd596",
   "metadata": {},
   "source": [
    "When dealing with an imbalanced dataset where most customers report satisfaction in your project, here are methods to balance the data and down-sample the majority class for estimating customer satisfaction:\n",
    "\n",
    "Down-sampling Techniques:\n",
    "\n",
    "Random sampling: This is the simplest approach. You randomly select a subset of the majority class data points (satisfied customers) to match the size of the minority class (dissatisfied customers). This is quick and easy to implement, but it discards potentially valuable data.\n",
    "\n",
    "Stratified sampling: This method ensures the distribution of features within the down-sampled majority class mirrors the original distribution. This preserves the representativeness of the majority class in the reduced sample and is preferred over random sampling.\n",
    "\n",
    "NearMiss sampling: This technique selects majority class data points that are most similar to the minority class data points based on specific features (e.g., product type, purchase history). This helps maintain the characteristics of the minority class while down-sampling the majority class.\n",
    "\n",
    "Additional Considerations:\n",
    "\n",
    "Data size: The amount of data you have available will influence the feasibility of down-sampling. With very large datasets, losing some data through down-sampling might be acceptable. However, for smaller datasets, consider exploring other options like up-sampling the minority class or using techniques like cost-sensitive learning that penalize misclassifying the minority class more heavily.\n",
    "\n",
    "Evaluation metrics: Choose appropriate metrics to evaluate your model's performance on an imbalanced dataset, such as precision, recall, F1-score, or AUC-ROC, as explained in Q9.\n",
    "\n",
    "Alternatives to Down-sampling:\n",
    "\n",
    "Up-sampling: This involves creating synthetic data points for the minority class (dissatisfied customers). Techniques like SMOTE can be used, but be cautious of overfitting when using this approach.\n",
    "\n",
    "Cost-sensitive learning: As mentioned previously, this method assigns higher weights to misclassifying the minority class, encouraging the model to prioritize accurate identification of dissatisfied customers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfe4139-1650-40ed-b538-e1082ff99f6f",
   "metadata": {},
   "source": [
    "## Q11: You discover that the dataset is unbalanced with a low percentage of occurrences while working on a project that requires you to estimate the occurrence of a rare event. What methods can you employ to balance the dataset and up-sample the minority class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141352f8-4dd5-4e5c-8614-3d0e74cc48d0",
   "metadata": {},
   "source": [
    "Since you're working with a rare event and need to estimate its occurrence in your project, up-sampling the minority class is a suitable approach to balance your dataset. Here are some methods you can employ:\n",
    "\n",
    "Up-sampling Techniques:\n",
    "\n",
    "Random oversampling: This is the simplest method, where you simply duplicate existing data points from the minority class (rare events) until it reaches the desired size. It's easy to implement but can lead to overfitting because the model learns from repeated examples.\n",
    "\n",
    "SMOTE (Synthetic Minority Oversampling Technique): This technique generates synthetic data points for the minority class based on existing data. It creates new data points by interpolating between existing minority class samples, addressing the issue of overfitting present in random oversampling.\n",
    "\n",
    "ADASYN (Adaptive Synthetic Minority Oversampling Technique): This is an advanced version of SMOTE that takes into account the density of the minority class data. It focuses on generating new data points in areas with lower density, leading to a more balanced distribution within the minority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3769c9-df7c-4edc-bd6b-07b038f3098b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "33c59435-df9d-434f-b029-8ab360ea7101",
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4ab7fe-bd2f-43b0-94cc-ff5e19b89c8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
