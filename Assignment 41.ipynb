{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e37b0f92-929d-4eb4-9701-aaebad078436",
   "metadata": {},
   "source": [
    "## Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357dd184-68b8-4e0e-9316-7ac4673096a0",
   "metadata": {},
   "source": [
    "Min-Max scaling, also known as Min-Max normalization, is a data preprocessing technique used to scale numerical features within a specific range, typically between 0 and 1 or -1 and 1. This is done to improve the performance of machine learning algorithms that are sensitive to the scale of the input data.\n",
    "\n",
    "Imagine you have a dataset with two features: age (ranging from 20 to 60 years) and annual income (ranging from $20,000 to $150,000). If you use these features directly in a machine learning model, the model might be biased towards the income feature due to its larger scale.\n",
    "\n",
    "By applying Min-Max scaling:\n",
    "\n",
    "Scaled age: (Age - 20) / (60 - 20) => Values will range from 0 to 1.\n",
    "Scaled income: (Income - 20,000) / (150,000 - 20,000) => Values will range from 0 to 1.\n",
    "Now, both features have the same scale (between 0 and 1), and the model won't be biased towards any specific feature based on its original scale.\n",
    "\n",
    "Limitations to consider:\n",
    "\n",
    "Sensitive to outliers: Outliers can significantly affect the minimum and maximum values, leading to distorted scaling. Consider handling outliers before applying Min-Max scaling.\n",
    "Loss of information: The original data distribution might be lost during scaling, potentially impacting the interpretability of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b4e2770-6f7c-4225-be59-2cbdd8d0d9bc",
   "metadata": {},
   "source": [
    "## Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe7f5fed-f115-4a28-8d92-02504edb83c3",
   "metadata": {},
   "source": [
    "The Unit Vector technique scales features by normalizing the entire data point to have a unit length (L2 norm). It transforms each data point into a unit vector on the hypersphere.\n",
    "\n",
    "Differences from Min-Max scaling:\n",
    "\n",
    "Range: Unit Vector scales to unit length, not a specific range like 0-1 in Min-Max.\n",
    "Focus: Unit Vector normalizes the entire data point, while Min-Max scales individual features.\n",
    "Example:\n",
    "\n",
    "Consider a 2D point (3, 4). Its unit vector would be (0.6, 0.8). Both points represent the same direction but on different scales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21dbe067-fb92-43b4-91a7-cd6d8b018762",
   "metadata": {},
   "source": [
    "## Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an example to illustrate its application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886313c2-d4d3-4e2b-889e-2e70d6151909",
   "metadata": {},
   "source": [
    "PCA (Principal Component Analysis) is a dimensionality reduction technique that identifies a new set of uncorrelated features, called principal components (PCs), that capture the most variance in the data. These PCs are ordered by their explained variance, allowing you to select the most informative ones while discarding less important features.\n",
    "\n",
    "How it works:\n",
    "\n",
    "Center the data: Subtract the mean from each feature.\n",
    "\n",
    "Calculate the covariance matrix: This captures the linear relationships between features.\n",
    "\n",
    "Eigenvalue decomposition: Decomposes the covariance matrix into eigenvectors (representing directions of greatest variance) and eigenvalues (indicating the variance explained by each direction).\n",
    "\n",
    "Select PCs: Choose the top k eigenvectors corresponding to the highest eigenvalues, where k is the desired number of dimensions.\n",
    "\n",
    "Project data: Transform the data onto the chosen principal components to obtain the reduced-dimensionality representation.\n",
    "\n",
    "Example:\n",
    "\n",
    "Imagine you have a dataset with features like height, weight, and arm span. PCA might reveal that the first principal component captures most of the variance, representing a combined effect of height and arm span. The second component might capture weight-related information. By selecting these top PCs, you can effectively reduce dimensionality while retaining the most important information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b145e5b1-3fb8-4e57-99ed-735a3bc4d60b",
   "metadata": {},
   "source": [
    "## Q4. What is the relationship between PCA and Feature Extraction, and how can PCA be used for Feature Extraction? Provide an example to illustrate this concept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deb03b3-d0a9-4ad0-8074-c05b8e088dea",
   "metadata": {},
   "source": [
    "Identifying uncorrelated features: These features, called principal components (PCs), capture the most variance in the data, representing the most informative directions.\n",
    "\n",
    "Discarding less important information: By selecting only the top PCs, you can effectively reduce the dimensionality of the data while retaining the most important features for capturing the underlying structure and relationships.\n",
    "\n",
    "Example:\n",
    "\n",
    "Imagine you have a dataset with image features like pixel intensities. Applying PCA might reveal that the first few PCs capture most of the image's visual information (e.g., edges, shapes). By selecting these top PCs, you can extract the most relevant features for tasks like image recognition, discarding redundant or less informative pixel data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc42e494-db6d-43f1-ac65-d6e3a40b776f",
   "metadata": {},
   "source": [
    "## Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to preprocess the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9133552-9c3d-4359-b2cf-59c67c9b07e5",
   "metadata": {},
   "source": [
    "Identify the minimum and maximum values for each feature (price, rating, delivery time) in your dataset.\n",
    "\n",
    "For each data point (representing a food item):\n",
    "\n",
    "Scale the price:\n",
    "Subtract the minimum price from the original price.\n",
    "\n",
    "Divide the result by the difference between the maximum and minimum prices.\n",
    "\n",
    "Scale the rating:\n",
    "\n",
    "Follow the same steps as for price, using the minimum and maximum rating values.\n",
    "\n",
    "Scale the delivery time:\n",
    "\n",
    "Follow the same steps as for price, using the minimum and maximum delivery times."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452178a5-ef10-48d2-85a8-258b163378cb",
   "metadata": {},
   "source": [
    "## Q6. You are working on a project to build a model to predict stock prices. The dataset contains many features, such as company financial data and market trends. Explain how you would use PCA to reduce the dimensionality of the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049f0105-d663-4e17-914e-d887bf2fffef",
   "metadata": {},
   "source": [
    "Preprocess the data: Handle missing values, outliers, and ensure features are on a similar scale (consider standardization if necessary).\n",
    "\n",
    "Center the data: Subtract the mean from each feature to remove bias.\n",
    "\n",
    "Calculate the covariance matrix: This captures the linear relationships between features.\n",
    "\n",
    "Perform eigenvalue decomposition: Decompose the covariance matrix to obtain eigenvectors (representing directions of greatest variance) and \n",
    "eigenvalues (indicating the variance explained by each direction).\n",
    "\n",
    "Select the top k principal components (PCs): Choose the PCs corresponding to the highest eigenvalues, representing the most significant directions of variance in the data. The number of PCs (k) can be determined based on a desired explained variance threshold or using techniques like the scree plot.\n",
    "\n",
    "Project the data: Transform the original data points onto the chosen PCs to obtain the reduced-dimensionality representation. This new representation captures the most important information from the original features while discarding redundant or less informative dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d35e3be-1681-4946-b0e3-7b5b5589e8df",
   "metadata": {},
   "source": [
    "## Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the values to a range of -1 to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed59f984-cab5-4135-a6b5-52c21bf377c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b3f2a24f-84d4-43bd-8ded-7c5f25299c30",
   "metadata": {},
   "source": [
    "## Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064f9f2a-0caa-4ff8-a971-44f0f68ee36b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
