{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c99093a-0c35-4e78-aabd-6ebf5a5589d4",
   "metadata": {},
   "source": [
    "Q1. What is an ensemble technique in machine learning?\n",
    "\n",
    "Q2. Why are ensemble techniques used in machine learning?\n",
    "\n",
    "Q3. What is bagging?\n",
    "\n",
    "Q4. What is boosting?\n",
    "\n",
    "Q5. What are the benefits of using ensemble techniques?\n",
    "\n",
    "Q6. Are ensemble techniques always better than individual models?\n",
    "\n",
    "Q7. How is the confidence interval calculated using bootstrap?\n",
    "\n",
    "Q8. How does bootstrap work and What are the steps involved in bootstrap?\n",
    "\n",
    "Q9. A researcher wants to estimate the mean height of a population of trees. They measure the height of a\n",
    "sample of 50 trees and obtain a mean height of 15 meters and a standard deviation of 2 meters. Use\n",
    "bootstrap to estimate the 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f05afebb-ba84-4b7e-b225-22090b9563c0",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "035c0475-5037-4774-9969-4d526675a107",
   "metadata": {},
   "source": [
    "Q1. Ensemble Technique:\n",
    "\n",
    "An ensemble technique combines multiple machine learning models to improve generalization (performance on unseen data) and robustness (resistance to errors in individual models).\n",
    "\n",
    "Q2. Reasons for Using Ensemble Techniques:\n",
    "\n",
    "\n",
    "Improved performance: Ensembles often outperform individual models by leveraging the strengths of different models and reducing variance.\n",
    "\n",
    "Reduced overfitting: Combining diverse models can help prevent overfitting to the training data.\n",
    "\n",
    "Increased model robustness: Averaging or voting over multiple models can provide a more robust prediction than relying on a single model.\n",
    "\n",
    "Q3. Bagging (Bootstrap Aggregation):\n",
    "\n",
    "Trains** multiple** models independently on random subsets (with replacement) of the original data.\n",
    "\n",
    "Final prediction is often the average (regression) or majority vote (classification) of individual predictions.\n",
    "\n",
    "Popular bagging algorithms include Random Forest and Extra Trees.\n",
    "\n",
    "Q4. Boosting:\n",
    "\n",
    "Trains models sequentially, where each model learns from the errors of the previous model.\n",
    "\n",
    "Focuses on improving the performance for previously misclassified data points.\n",
    "\n",
    "Popular boosting algorithms include AdaBoost and Gradient Boosting.\n",
    "\n",
    "Q5. Benefits of Ensemble Techniques:\n",
    "\n",
    "Improved accuracy and generalization.\n",
    "\n",
    "Reduced overfitting and increased robustness.\n",
    "\n",
    "Can handle complex and non-linear relationships in data.\n",
    "\n",
    "Often provide more informative feature importance analysis.\n",
    "\n",
    "Q6. Are Ensemble Techniques Always Better?\n",
    "\n",
    "Not necessarily. They can be computationally expensive to train and interpret.\n",
    "\n",
    "Individual models might still be better suited for specific tasks, especially when interpretability or simplicity is crucial.\n",
    "\n",
    "Q7. Confidence Interval with Bootstrap:\n",
    "\n",
    "Bootstrap resamples the original data with replacement to create multiple \"bootstrapped samples\" (usually 1000 or more).\n",
    "\n",
    "For each bootstrapped sample, calculate the desired statistic (e.g., mean height).\n",
    "\n",
    "The confidence interval is based on the distribution of the statistic across all bootstrapped samples.\n",
    "\n",
    "Q8. Bootstrap Steps:\n",
    "\n",
    "Sample with replacement: Draw a sample of the same size as the original data with replacement from the original data. This allows some data points to be chosen multiple times and others to be excluded.\n",
    "\n",
    "Repeat step 1: Repeat step 1 a large number of times (e.g., 1000) to create multiple bootstrapped samples.\n",
    "\n",
    "Calculate statistic: For each bootstrapped sample, calculate the desired statistic (e.g., mean height).\n",
    "\n",
    "Confidence interval: Determine the desired confidence level (e.g., 95%). Find the percentiles (e.g., 2.5% and 97.5%) in the distribution of the \n",
    "statistic across all bootstrapped samples. This represents the lower and upper bounds of the confidence interval.\n",
    "\n",
    "Q9. Bootstrap for Confidence Interval:\n",
    "\n",
    "Sample 1000 times with replacement from your original sample of 50 tree heights.\n",
    "\n",
    "Calculate the mean height for each of the 1000 bootstrapped samples.\n",
    "\n",
    "The 2.5th percentile and 97.5th percentile of the resulting distribution of means will represent the lower and upper bounds of your 95% confidence interval for the population mean height."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbe56bf-7844-4bf2-80c0-5d18bdfeed29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
