{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a7bf87b8-6d02-4301-8979-ccda14933df3",
   "metadata": {},
   "source": [
    "Q1. What is Random Forest Regressor?\n",
    "\n",
    "Q2. How does Random Forest Regressor reduce the risk of overfitting?\n",
    "\n",
    "Q3. How does Random Forest Regressor aggregate the predictions of multiple decision trees?\n",
    "\n",
    "Q4. What are the hyperparameters of Random Forest Regressor?\n",
    "\n",
    "Q5. What is the difference between Random Forest Regressor and Decision Tree Regressor?\n",
    "\n",
    "Q6. What are the advantages and disadvantages of Random Forest Regressor?\n",
    "\n",
    "Q7. What is the output of Random Forest Regressor?\n",
    "\n",
    "Q8. Can Random Forest Regressor be used for classification tasks?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1bb0df5-54af-408c-bfa7-349f23fcac0b",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "122a356f-0bd5-4230-872f-21aae33c4650",
   "metadata": {},
   "source": [
    "Q1. Random Forest Regressor:\n",
    "\n",
    "Random Forest Regressor is an ensemble learning algorithm that utilizes multiple decision trees for regression tasks. It combines the predictions of multiple trees to achieve a more robust and accurate prediction.\n",
    "\n",
    "Q2. Reducing Overfitting:\n",
    "\n",
    "Random Forest reduces the risk of overfitting in several ways:\n",
    "\n",
    "Bagging: It uses bootstrapping to create diverse training datasets for each tree. This prevents any single tree from overfitting to specific patterns in the training data.\n",
    "\n",
    "Feature Randomness: During each tree's growth, only a random subset of features is considered for splitting at each node. This forces the trees to learn different representations of the data and reduces variance.\n",
    "\n",
    "Q3. Prediction Aggregation:\n",
    "\n",
    "Random Forest aggregates individual tree predictions by averaging them. This final prediction is often closer to the true value than any single tree's prediction due to the averaging effect and reduced overfitting.\n",
    "\n",
    "Q4. Hyperparameters:\n",
    "\n",
    "n_estimators: Number of decision trees in the forest (affects variance and computational cost).\n",
    "\n",
    "max_depth: Maximum depth of each tree (affects bias and variance).\n",
    "\n",
    "min_samples_split: Minimum number of samples required to split a node (affects bias and variance).\n",
    "\n",
    "min_samples_leaf: Minimum number of samples required at a leaf node (affects variance).\n",
    "\n",
    "max_features: Number of features randomly considered at each split (affects variance).\n",
    "\n",
    "Q5. Random Forest vs. Decision Tree Regression:\n",
    "\n",
    "Feature\tRandom Forest Regressor\tDecision Tree Regressor\n",
    "\n",
    "Model type\tEnsemble learning\tSingle decision tree\n",
    "\n",
    "Overfitting\tLower risk due to bagging and feature randomness\tHigher risk\n",
    "\n",
    "Performance\tGenerally better due to reduced variance and overfitting\tCan be good for simple problems, prone to overfitting for complex ones\n",
    "\n",
    "Interpretability\tLess interpretable due to multiple trees\tMore interpretable due to single tree structure\n",
    "\n",
    "Q6. Advantages and Disadvantages:\n",
    "\n",
    "Advantages:\n",
    "\n",
    "High accuracy and generalizability: Often outperforms individual decision trees due to reduced overfitting.\n",
    "\n",
    "Robust to outliers and noise: Less sensitive to individual data points due to averaging.\n",
    "\n",
    "Handles missing values: Can handle missing data by imputing or ignoring missing features during splits.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "Less interpretable: Understanding individual tree logic can be challenging.\n",
    "\n",
    "Computationally expensive: Training multiple decision trees can be computationally intensive.\n",
    "\n",
    "Q7. Output:\n",
    "\n",
    "Random Forest Regressor outputs a single continuous value (prediction) for a given input sample.\n",
    "\n",
    "Q8. Classification Tasks:\n",
    "\n",
    "No, Random Forest Regressor specifically deals with regression tasks (predicting continuous values). However, a similar ensemble method called Random Forest Classifier can be used for classification tasks (predicting discrete classes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c170a57c-15da-4399-8517-c674cf40a9e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
