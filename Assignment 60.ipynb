{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91c0d972-5f3f-4d51-8c96-974801915567",
   "metadata": {},
   "source": [
    "Q1. You are working on a machine learning project where you have a dataset containing numerical and\n",
    "categorical features. You have ident#fied that some of the features are highly correlated and there are\n",
    "missing values in some of the columns. You want to build a pipeline that automates the feature\n",
    "engineering process and handles the missing valuesD\n",
    "\n",
    "Design a pipeline that includes the following steps\"\n",
    "\n",
    "Use an automated feature selection method to identify the important features in the datasetC\n",
    "\n",
    "Create a numerical pipeline that includes the following steps\"\n",
    "\n",
    "Impute the missing values in the numerical columns using the mean of the column valuesC\n",
    "\n",
    "Scale the numerical columns using standardisationC\n",
    "\n",
    "Create a categorical pipeline that includes the following steps\"\n",
    "\n",
    "Impute the missing values in the categorical columns using the most frequent value of the columnC\n",
    "\n",
    "One-hot encode the categorical columnsC\n",
    "\n",
    "Combine the numerical and categorical pipelines using a ColumnTransformerC\n",
    "\n",
    "Use a Random Forest Classifier to build the final modelC\n",
    "\n",
    "Evaluate the accuracy of the model on the test datasetD\n",
    "\n",
    "Note! Your solution should include code snippets for each step of the pipeline, and a brief explanation of\n",
    "each step. You should also provide an interpretation of the results and suggest possible improvements for\n",
    "the pipelineD\n",
    "\n",
    "Q2. Build a pipeline that includes a random forest classifier and a logistic regression classifier, and then\n",
    "use a voting classifier to combine their predictions. Train the pipeline on the iris dataset and evaluate its\n",
    "accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da2f0c3-e3b1-4494-a846-0a1b14e19db1",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1885a8-a8d5-4620-81d6-19f58302e826",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Q1: Feature Engineering Pipeline:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6b25be-a524-4c88-8487-0edf508a4501",
   "metadata": {},
   "source": [
    "# Step 1: Automated Feature Selection\n",
    "feature_selection = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "\n",
    "# Explanation: SelectFromModel uses a machine learning model (RandomForestClassifier in this case) to automatically select important features based on their importance scores.\n",
    "\n",
    "# Step 2: Numerical Pipeline\n",
    "numerical_features = X.select_dtypes(include=['int64', 'float64']).columns\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Explanation: Impute missing values with the mean and standardize numerical features to ensure consistent scales for model training.\n",
    "\n",
    "# Step 3: Categorical Pipeline\n",
    "categorical_features = X.select_dtypes(include=['object']).columns\n",
    "categorical_pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('encoder', OneHotEncoder())\n",
    "])\n",
    "\n",
    "# Explanation: Impute missing values with the most frequent value and one-hot encode categorical features to convert them into a format suitable for machine learning models.\n",
    "\n",
    "# Step 4: Combine Numerical and Categorical Pipelines\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('cat', categorical_pipeline, categorical_features)\n",
    "    ])\n",
    "\n",
    "# Explanation: Use ColumnTransformer to apply different preprocessing steps to numerical and categorical features.\n",
    "\n",
    "# Step 5: Final Model Pipeline\n",
    "model_pipeline = Pipeline([\n",
    "    ('feature_selection', feature_selection),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Explanation: Combine the feature selection, preprocessing, and the RandomForestClassifier into a final pipeline.\n",
    "\n",
    "# Step 6: Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Explanation: Split the dataset into training and test sets for model evaluation.\n",
    "\n",
    "# Step 7: Fit and evaluate the model\n",
    "model_pipeline.fit(X_train, y_train)\n",
    "y_pred = model_pipeline.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Explanation: Fit the model on the training data and evaluate its accuracy on the test data.\n",
    "\n",
    "# Interpretation of Results:\n",
    "# The accuracy score obtained on the test set provides an indication of the model's performance. Higher accuracy suggests better predictive capability.\n",
    "\n",
    "# Possible Improvements for Pipeline:\n",
    "# 1. Grid search for hyperparameter tuning in the RandomForestClassifier.\n",
    "# 2. Consider other feature selection methods for experimentation.\n",
    "# 3. Evaluate the impact of different imputation strategies for missing values.\n",
    "# 4. Explore additional preprocessing steps or try different machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fde2e39-0b07-4cfe-8fcd-8615eee57903",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q2: Pipeline with Voting Classifier on Iris Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "647c00fa-0d06-4d14-81a2-14c1c258951b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Step 1: Load and split the Iris dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Step 2: Create individual classifiers\n",
    "rf_classifier = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "lr_classifier = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('classifier', LogisticRegression(random_state=42))\n",
    "])\n",
    "\n",
    "# Step 3: Combine classifiers using a Voting Classifier\n",
    "voting_classifier = VotingClassifier(estimators=[\n",
    "    ('rf', rf_classifier),\n",
    "    ('lr', lr_classifier)\n",
    "], voting='hard')\n",
    "\n",
    "# Step 4: Fit and evaluate the model\n",
    "voting_classifier.fit(X_train, y_train)\n",
    "y_pred = voting_classifier.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bfa77a-4582-4211-9b98-458ee44a8828",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
