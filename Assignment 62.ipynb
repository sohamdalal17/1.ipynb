{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6a3960c-dc71-4eba-9dd9-288fca9fa672",
   "metadata": {},
   "source": [
    "Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters\n",
    "\n",
    "Q4. What is a weak learner in Gradient Boosting?\n",
    "\n",
    "Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "\n",
    "Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "\n",
    "Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f83f240-3687-4b09-ba5e-4ebd4a629568",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbdadb1-f9d8-4f5f-b9b7-947fcca75316",
   "metadata": {},
   "source": [
    "Q1. Gradient Boosting Regression:\n",
    "\n",
    "Gradient Boosting Regression is an ensemble learning technique that uses sequential model fitting to improve prediction accuracy for regression tasks. It starts with a simple model (weak learner) and iteratively adds new models that focus on correcting the errors made by the previous ones.\n",
    "\n",
    "Q3. Hyperparameter Tuning:\n",
    "\n",
    "Grid search or random search can be used to find optimal hyperparameters (learning rate, number of trees, tree depth) for your specific data. This involves trying different combinations of values and evaluating the performance on a validation set.\n",
    "\n",
    "Q4. Weak Learner in Gradient Boosting:\n",
    "\n",
    "In Gradient Boosting, a weak learner is a simple model, often a decision tree with limited depth or other restrictions. These individual models may not be very accurate on their own, but when combined, they can produce a strong ensemble model with improved performance.\n",
    "\n",
    "Q5. Intuition behind Gradient Boosting:\n",
    "\n",
    "The intuition behind Gradient Boosting is to gradually improve predictions by focusing on the errors made by previous models. It works as follows:\n",
    "\n",
    "Start with a simple model and predict the target values.\n",
    "\n",
    "Calculate the residuals (difference between predicted and actual values).\n",
    "\n",
    "Train a new weak learner to predict these residuals.\n",
    "\n",
    "Combine the predictions of the new learner and the previous model, typically using a weighted sum.\n",
    "\n",
    "Repeat steps 2-4 until a stopping criterion is met (e.g., maximum number of learners or desired performance level).\n",
    "\n",
    "Q6. Building Ensemble:\n",
    "\n",
    "Gradient Boosting builds an ensemble by iteratively adding weak learners:\n",
    "\n",
    "In each iteration, the model focuses on the errors (residuals) from the previous model.\n",
    "\n",
    "A new weak learner is trained to predict these residuals, essentially learning how to improve upon the existing model's shortcomings.\n",
    "\n",
    "The predictions of all weak learners are combined to form the final ensemble prediction.\n",
    "\n",
    "Q7. Mathematical Intuition:\n",
    "\n",
    "The mathematical details of Gradient Boosting involve minimizing a loss function (e.g., mean squared error) using gradient descent techniques. This requires calculating the gradients of the loss function with respect to the predictions of the model. These gradients provide guidance on how to adjust the predictions in each iteration to minimize the overall error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5c004391-286b-4044-b0cb-d150f3a4283c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 2.4619\n",
      "R-squared: -0.1255\n"
     ]
    }
   ],
   "source": [
    "# Q2. \n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "def gradient_boosting(X, y, learning_rate, n_estimators, max_depth=3):\n",
    " \n",
    "  predictions = np.zeros(len(y))\n",
    "  for i in range(n_estimators):\n",
    "   \n",
    "    residuals = y - predictions\n",
    "\n",
    "    weak_learner = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    weak_learner.fit(X, residuals)\n",
    "\n",
    "    predictions += learning_rate * weak_learner.predict(X)\n",
    "\n",
    "  return predictions\n",
    "\n",
    "X = np.array([[1], [2], [3], [4]])\n",
    "y = np.array([2, 4, 5, 6])\n",
    "\n",
    "predictions = gradient_boosting(X, y, learning_rate=0.1, n_estimators=10)\n",
    "\n",
    "mse = np.mean((y - predictions) ** 2)\n",
    "r2 = 1 - np.sum((y - predictions) ** 2) / np.sum((y - np.mean(y)) ** 2)\n",
    "\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")\n",
    "print(f\"R-squared: {r2:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fd162c1-3c61-41be-b90d-f4992c30835d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
