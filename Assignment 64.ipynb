{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23eb1abd-f3ba-427f-938d-e56ec36cd660",
   "metadata": {},
   "source": [
    "Q1. What is the main difference between the Euclidean distance metric and the Manhattan distance metric in KNN? How might this difference affect the performance of a KNN classifier or regressor?\n",
    "\n",
    "Q2. How do you choose the optimal value of k for a KNN classifier or regressor? What techniques can be used to determine the optimal k value?\n",
    "\n",
    "Q3. How does the choice of distance metric affect the performance of a KNN classifier or regressor? In what situations might you choose one distance metric over the other?\n",
    "\n",
    "Q4. What are some common hyperparameters in KNN classifiers and regressors, and how do they affect the performance of the model? How might you go about tuning these hyperparameters to improvemodel performance?\n",
    "\n",
    "Q5. How does the size of the training set affect the performance of a KNN classifier or regressor? What techniques can be used to optimize the size of the training set?\n",
    "\n",
    "Q6. What are some potential drawbacks of using KNN as a classifier or regressor? How might you overcome these drawbacks to improve the performance of the model?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803360b3-560f-407e-9a37-7b1e79097868",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb0403fa-aa9b-4846-9b12-497f790c55a6",
   "metadata": {},
   "source": [
    "Q1. Euclidean vs. Manhattan Distance:\n",
    "\n",
    "Euclidean: Straight-line distance, penalizes large deviations more heavily.\n",
    "\n",
    "Manhattan: Sum of absolute differences along each dimension, less sensitive to outliers.\n",
    "\n",
    "Impact:\n",
    "\n",
    "Euclidean: Can be affected by outliers, may favor features with larger scales. Better for data with spherical clusters.\n",
    "\n",
    "Manhattan: Less sensitive to outliers, treats all features equally. Better for data with non-spherical or grid-like patterns.\n",
    "\n",
    "Q2. Choosing the optimal k:\n",
    "\n",
    "No single optimal value, needs experimentation.\n",
    "\n",
    "Techniques:\n",
    "\n",
    "Cross-validation: Split data into folds, train on different folds, evaluate on remaining folds, repeat, and average performance for different k values.\n",
    "\n",
    "Grid search: Try a range of k values and choose the one with the best performance metric.\n",
    "\n",
    "Q3. Distance metric vs. performance:\n",
    "\n",
    "Choice of metric can impact performance significantly.\n",
    "\n",
    "Consider data characteristics:\n",
    "\n",
    "Euclidean: Suitable for continuous, spherical data.\n",
    "\n",
    "Manhattan: Suitable for non-negative data, grid-like patterns, or less sensitive to outliers.\n",
    "\n",
    "Q4. Hyperparameters in KNN:\n",
    "\n",
    "k: Number of neighbors (discussed in Q2).\n",
    "\n",
    "Distance metric: Euclidean, Manhattan, etc. (discussed in Q1 & Q3).\n",
    "\n",
    "Tuning hyperparameters:\n",
    "\n",
    "Grid search or random search: Evaluate performance across different hyperparameter combinations.\n",
    "\n",
    "Visualization techniques: Plot performance metrics for different values to help identify optimal settings.\n",
    "\n",
    "Q5. Training set size and KNN:\n",
    "\n",
    "Larger training set generally leads to better performance.\n",
    "\n",
    "Optimizing size:\n",
    "\n",
    "Use techniques like active learning to identify informative data points.\n",
    "\n",
    "Consider data augmentation to artificially increase data diversity.\n",
    "\n",
    "Q6. Drawbacks and improvements:\n",
    "\n",
    "Drawbacks:\n",
    "\n",
    "High computational cost for large datasets and k.\n",
    "\n",
    "Sensitive to irrelevant features.\n",
    "\n",
    "Curse of dimensionality.\n",
    "\n",
    "Improvements:\n",
    "\n",
    "Use efficient data structures for faster neighbor search.\n",
    "\n",
    "Perform feature selection to identify relevant features.\n",
    "\n",
    "Consider dimensionality reduction techniques for high-dimensional data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8701b4cc-2238-42db-98f9-b3d49ffc048c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
