{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ea55c9-885a-4651-89f0-85bf206b1f4f",
   "metadata": {},
   "source": [
    "Q1. What is a contingency matrix, and how is it used to evaluate the performance of a classification model?\n",
    "\n",
    "Q2. How is a pair confusion matrix different from a regular confusion matrix, and why might it be useful in certain situations?\n",
    "\n",
    "Q3. What is an extrinsic measure in the context of natural language processing, and how is it typically used to evaluate the performance of language models?\n",
    "\n",
    "Q4. What is an intrinsic measure in the context of machine learning, and how does it differ from an extrinsic measure?\n",
    "\n",
    "Q5. What is the purpose of a confusion matrix in machine learning, and how can it be used to identify strengths and weaknesses of a model?\n",
    "\n",
    "Q6. What are some common intrinsic measures used to evaluate the performance of unsupervised learning algorithms, and how can they be interpreted?\n",
    "\n",
    "Q7. What are some limitations of using accuracy as a sole evaluation metric for classification tasks, and how can these limitations be addressed?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fb43a5-ac35-4896-aec9-99f2da5dfa84",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e918c-aed1-4d72-8b83-24eddebf004f",
   "metadata": {},
   "source": [
    "Q1. Contingency Matrix and Classification Evaluation:\n",
    "\n",
    "A contingency matrix (also called a confusion matrix) is a table that summarizes the predicted vs. actual labels for a classification model.\n",
    "\n",
    "It helps evaluate the model's performance by showing:\n",
    "\n",
    "True positives (TP): Correctly predicted positive cases.\n",
    "\n",
    "True negatives (TN): Correctly predicted negative cases.\n",
    "\n",
    "False positives (FP): Incorrectly predicted positive cases (Type I error).\n",
    "\n",
    "False negatives (FN): Incorrectly predicted negative cases (Type II error).\n",
    "\n",
    "Q2. Pair Confusion Matrix:\n",
    "\n",
    "A pair confusion matrix extends the concept to multi-class classification, comparing predictions and actual labels for each possible pair of classes.\n",
    "\n",
    "It's useful when:\n",
    "\n",
    "Analyzing confusions between specific class pairs (e.g., identifying which classes the model struggles to distinguish).\n",
    "\n",
    "Evaluating models with many classes where a single confusion matrix becomes overwhelming.\n",
    "\n",
    "Q3. Extrinsic Measures in NLP:\n",
    "\n",
    "Extrinsic measures evaluate the indirect impact of a language model on a downstream task, measuring how well it improves performance on a \n",
    "related application.\n",
    "\n",
    "Examples:\n",
    "\n",
    "Machine translation evaluation by comparing translated text quality to human translation.\n",
    "\n",
    "Question answering system evaluation by measuring the accuracy of retrieved answers to factual questions.\n",
    "\n",
    "Q4. Intrinsic vs. Extrinsic Measures:\n",
    "\n",
    "Intrinsic measures evaluate a model's performance on the task it was specifically trained for, using measures like accuracy, precision, recall, F1-score, etc., based on the ground truth labels provided during training.\n",
    "\n",
    "Extrinsic measures evaluate how well the model's learned representations or outputs benefit other tasks, not directly related to the training objective.\n",
    "\n",
    "Q5. Confusion Matrix and Model Strengths & Weaknesses:\n",
    "\n",
    "A confusion matrix helps identify:\n",
    "\n",
    "Overall accuracy: (TP + TN) / (Total)\n",
    "\n",
    "Precision: TP / (TP + FP) (measures how many positive predictions were actually correct)\n",
    "\n",
    "Recall: TP / (TP + FN) (measures how many actual positive cases were identified correctly)\n",
    "\n",
    "False positive rate (FPR): FP / (Total Negatives) (measures the rate of misclassifying negative cases)\n",
    "\n",
    "False negative rate (FNR): FN / (Total Positives) (measures the rate of missing actual positive cases)\n",
    "\n",
    "By analyzing these metrics, you can understand the model's strengths (e.g., high accuracy in specific classes) and weaknesses (e.g., high false positives for a particular class).\n",
    "\n",
    "Q6. Intrinsic Measures for Unsupervised Learning:\n",
    "\n",
    "Common measures:\n",
    "\n",
    "Silhouette Coefficient: Measures cluster separation and cohesion (supervised or unsupervised).\n",
    "\n",
    "Calinski-Harabasz Index: Compares within-cluster variance to between-cluster variance (unsupervised).\n",
    "\n",
    "Davies-Bouldin Index: Similar to Calinski-Harabasz Index, but uses ratio of within-cluster scatter to inter-cluster separation (unsupervised).\n",
    "\n",
    "Interpretation: Higher values generally indicate better clustering quality (more compact and well-separated clusters).\n",
    "\n",
    "Q7. Limitations of Accuracy and Addressing them:\n",
    "\n",
    "Limitations:\n",
    "\n",
    "Accuracy can be misleading in imbalanced datasets where one class dominates.\n",
    "\n",
    "It doesn't capture class-specific performance (e.g., a model might be excellent at predicting the majority class but poor at identifying the minority class).\n",
    "\n",
    "Addressing limitations:\n",
    "\n",
    "Use precision, recall, and F1-score for a more balanced view.\n",
    "\n",
    "Consider class-specific metrics like precision-recall curves or ROC curves.\n",
    "\n",
    "Use domain knowledge to determine the most relevant metrics for your specific task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6c4bee-9523-42d3-9db7-cd97b1cd903d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
