{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dadf8786-c936-4e93-ab41-f083bcdde284",
   "metadata": {},
   "source": [
    "Q1. What is the mathematical formula for a linear SVM?\n",
    "\n",
    "Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "Q3. What is the kernel trick in SVM?\n",
    "\n",
    "Q4. What is the role of support vectors in SVM Explain with example\n",
    "\n",
    "Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?\n",
    "\n",
    "Q6. SVM Implementation through Iris dataset.\n",
    "\n",
    "Bonus task: Implement a linear SVM classifier from scratch using Python and compare its\n",
    "performance with the scikit-learn implementation.\n",
    "\n",
    "~ Load the iris dataset from the scikit-learn library and split it into a training set and a testing setl\n",
    "\n",
    "~ Train a linear SVM classifier on the training set and predict the labels for the testing setl\n",
    "\n",
    "~ Compute the accuracy of the model on the testing setl\n",
    "\n",
    "~ Plot the decision boundaries of the trained model using two of the featuresl\n",
    "\n",
    "~ Try different values of the regularisation parameter C and see how it affects the performance of\n",
    "the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b87b4f32-3c51-4820-84e4-9aa185a4ef11",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3832e14-447d-4c61-b751-9b03da10c8e8",
   "metadata": {},
   "source": [
    "Q1. Mathematical Formula for a Linear SVM:\n",
    "\n",
    "In linear SVM, the decision boundary is represented by a hyperplane defined by the equation:\n",
    "\n",
    "w^T * x + b = 0\n",
    "\n",
    "where:\n",
    "\n",
    "w: Weight vector (normal vector to the hyperplane)\n",
    "\n",
    "x: Feature vector of a data point\n",
    "\n",
    "b: Bias term\n",
    "\n",
    "Q2. Objective Function of a Linear SVM:\n",
    "\n",
    "The objective function minimizes two terms:\n",
    "\n",
    "Loss term: Penalizes misclassifications.\n",
    "\n",
    "Regularization term: Controls model complexity to prevent overfitting.\n",
    "\n",
    "The combined objective function:\n",
    "\n",
    "1/2 ||w||^2 + C * (sum of hinge losses for all data points)\n",
    "\n",
    "C: Regularization parameter, controlling the trade-off between margin maximization and training error.\n",
    "\n",
    "Q3. Kernel Trick in SVM:\n",
    "\n",
    "The kernel trick maps data points to a higher-dimensional feature space where a linear separation might be possible, even if the data is non-linearly separable in the original space.\n",
    "\n",
    "Q4. Role of Support Vectors:\n",
    "\n",
    "Support vectors (SVs) are the data points closest to the decision boundary (within the margin or on the margin itself). They define the hyperplane and most significantly influence its position.\n",
    "\n",
    "Example:\n",
    "\n",
    "Consider a two-dimensional dataset with two classes, represented by circles and squares, that are not linearly separable in the original space.\n",
    "\n",
    "Applying the kernel trick with a suitable kernel (e.g., polynomial kernel) can map the data points to a higher-dimensional space where linear separation becomes possible.\n",
    "\n",
    "In this higher-dimensional space, the support vectors will be the data points closest to the margins of the hyperplane, defining the decision boundary used for classification.\n",
    "\n",
    "Q5. Hyperplane, Marginal Plane, Soft Margin, and Hard Margin in SVM (with illustrations):\n",
    "\n",
    "Hyperplane:\n",
    "\n",
    "A separating plane that divides the data points into two classes.\n",
    "\n",
    "In linear SVM, the hyperplane is defined by the linear equation w^T * x + b = 0.\n",
    "\n",
    "Marginal Plane:\n",
    "\n",
    "Two parallel planes on either side of the hyperplane, with a distance of 2 * margin (denoted by 1/||w||) between them.\n",
    "\n",
    "These planes define the maximum margin, which the SVM aims to maximize.\n",
    "\n",
    "Soft Margin:\n",
    "\n",
    "Allows for a certain number of misclassified data points (outliers) within the margin, controlled by the regularization parameter C.\n",
    "\n",
    "This flexibility helps handle real-world datasets that may not be perfectly separable.\n",
    "\n",
    "Hard Margin:\n",
    "\n",
    "Aims for a perfect separation of data points (no misclassifications), where the margin is maximized.\n",
    "\n",
    "In practice, hard margins are often unattainable due to noise or non-linearity in real-world data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ef0492c4-56ce-4dc7-be12-be9e5f77e837",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# Q.6\n",
    "import numpy as np\n",
    "from sklearn import datasets, model_selection, svm\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = svm.SVC(kernel='linear')  \n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
