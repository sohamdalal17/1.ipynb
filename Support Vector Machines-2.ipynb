{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be080780-f3b9-4c35-a45c-3bf67ca43179",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?\n",
    "\n",
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "Q5. Assignment:\n",
    "L Import the necessary libraries and load the dataset\n",
    "\n",
    "L Split the dataset into training and testing setZ\n",
    "\n",
    "L Preprocess the data using any technique of your choice (e.g. scaling, normaliMationK\n",
    "\n",
    "L Create an instance of the SVC classifier and train it on the training datW\n",
    "\n",
    "L hse the trained classifier to predict the labels of the testing datW\n",
    "\n",
    "L Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-scoreK\n",
    "\n",
    "L Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performanc_\n",
    "\n",
    "L Train the tuned classifier on the entire dataseg\n",
    "\n",
    "L Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a88327-02d8-49aa-9109-cd71e3d8d63b",
   "metadata": {},
   "source": [
    "## Answers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad6f789-2b93-4d9d-8d18-b14bd0068505",
   "metadata": {},
   "source": [
    "Q1. Relationship between Polynomial Functions and Kernel Functions in Machine Learning:\n",
    "\n",
    "Polynomial functions: These functions are used in various machine learning tasks, including regression and classification. They represent relationships between features and outputs using terms like x^2, x^3, and so on.\n",
    "\n",
    "Kernel functions: In kernel methods, these functions transform data points from their original space to a potentially higher-dimensional feature space. \n",
    "\n",
    "The goal is to find a linear separation in this high-dimensional space, even if the data is non-linearly separable in the original space.\n",
    "\n",
    "Connection: The polynomial kernel function is one specific type of kernel function that uses polynomial terms to map data points. \n",
    "\n",
    "It essentially applies polynomial transformations to the features, similar to how polynomial functions represent relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dcdb0a4-2bb3-4811-83e1-5041b624410a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q.2\n",
    "import numpy as np\n",
    "from sklearn import datasets, model_selection, svm\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "clf = svm.SVC(kernel='poly', degree=3, gamma=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"F1-score:\", f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79062f60-845e-4165-9ee4-8b406fceeccd",
   "metadata": {},
   "source": [
    "Q3. Epsilon and Support Vectors in SVR:\n",
    "\n",
    "Epsilon (eps): This parameter controls the tolerance for errors in SVR. It defines the margin of error allowed around the regression line before data points are considered outliers.\n",
    "\n",
    "Impact on Support Vectors: Increasing epsilon reduces the number of support vectors. This is because more data points can now fall outside the margin of error, and only points closest to the estimated line remain as support vectors.\n",
    "\n",
    "Q4. SVR Hyperparameter Tuning:\n",
    "\n",
    "Kernel function:\n",
    "\n",
    "Choosing the right kernel function (e.g., linear, polynomial, RBF) depends on the underlying data distribution and problem type. \n",
    "\n",
    "Consider experimenting with different kernels to see which one performs best.\n",
    "\n",
    "C parameter:\n",
    "\n",
    "This parameter controls the trade-off between model complexity and fitting error. A higher C leads to a more complex model that might overfit, while a lower C might underfit. Tune C based on your validation set performance.\n",
    "Epsilon (eps):\n",
    "\n",
    "As explained earlier, eps controls the tolerance for errors. A smaller eps allows for less error but might make the model more sensitive to outliers. A larger eps allows for more flexibility but might lead to underfitting. The optimal value depends on \n",
    "your dataset's noise level and how strictly you need to fit the data.\n",
    "\n",
    "Gamma parameter (for RBF and other specific kernels):\n",
    "This parameter controls the influence of individual data points on the decision function (kernel shape) in the high-dimensional space. A higher gamma gives more local influence to data points, while a lower gamma makes the decision function smoother and less sensitive to local variations. Tune gamma based on your data's complexity and feature scales.\n",
    "\n",
    "Tuning Tips:\n",
    "\n",
    "Start with a reasonable estimate based on domain knowledge and experience.\n",
    "\n",
    "Use techniques like grid search or random search to systematically explore different hyperparameter combinations.\n",
    "\n",
    "Evaluate model performance on a validation set (not directly on the training set) to avoid overfitting.\n",
    "\n",
    "Choose the combination that provides the best performance on the validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ae4a0aa-a053-49b0-a724-d956048039fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features (X):\n",
      "[[0.69568234 0.27478178]\n",
      " [0.93089279 0.98847439]\n",
      " [0.37753798 0.60283341]\n",
      " [0.1223017  0.30887186]\n",
      " [0.21368358 0.24450605]\n",
      " [0.46839714 0.97676298]\n",
      " [0.78973368 0.32668638]\n",
      " [0.52382683 0.05013701]\n",
      " [0.28980911 0.63734967]\n",
      " [0.27907756 0.36874195]\n",
      " [0.95930037 0.05060058]\n",
      " [0.5559005  0.81241458]\n",
      " [0.02968495 0.10521753]\n",
      " [0.58243413 0.55918985]\n",
      " [0.59562834 0.9607371 ]\n",
      " [0.12239584 0.87069808]\n",
      " [0.47108926 0.57767061]\n",
      " [0.3362673  0.96029625]\n",
      " [0.67049803 0.96553655]\n",
      " [0.40173438 0.98435736]]\n",
      "\n",
      "Target labels (y):\n",
      "[0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.random.rand(20, 2)  \n",
    "y = np.array([0] * 10 + [1] * 10)\n",
    "\n",
    "# Print the sample data\n",
    "print(\"Features (X):\")\n",
    "print(X)\n",
    "print(\"\\nTarget labels (y):\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c00c32a-3c44-422d-8923-4fb208b35e36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Q.5\n",
    "import numpy as np\n",
    "from sklearn import model_selection, svm\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Data (replace with your actual data loading and preparation)\n",
    "X = np.array([[1, 2], [3, 1], [2, 5], [1, 4], [7, 2], [6, 1], [5, 5], [4, 4], [8, 1], [9, 2], [4, 1], [5, 0], [6, 3], [7, 4], [8, 5], [9, 6], [10, 1], [11, 2], [12, 3], [13, 4]])\n",
    "y = np.array([0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0])\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "clf = svm.SVC() \n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f23021-809f-42fb-b340-e8f39462c8d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
